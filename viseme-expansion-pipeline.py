#!/usr/bin/env python3
"""
Viseme Expansion Pipeline for Kelly & Ken Avatars
Expands from 12 to 48 visemes using training video frame extraction and AI enhancement
"""

import cv2
import numpy as np
import json
import os
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class VisemeExpansionPipeline:
    def __init__(self, base_path: str = "production-deploy/assets/avatars"):
        self.base_path = Path(base_path)
        self.training_videos = {
            'kelly': [
                '../critical-foundations/kelly2.mp4',
                '../critical-foundations/kelly2 (1).mp4'
            ],
            'ken': [
                '../critical-foundations/ken2.mp4', 
                '../critical-foundations/ken2 (1).mp4'
            ]
        }
        
        # Current 12 visemes
        self.current_visemes = [
            'REST', 'MBP', 'FV', 'TH', 'DNTL', 'KG', 
            'S', 'WQ', 'R', 'A', 'E', 'I'
        ]
        
        # New enhanced visemes to add
        self.new_visemes = [
            'OO', 'UH', 'AW', 'AY', 'OY', 'NG', 
            'CH', 'SH', 'ZH', 'Y', 'L', 'H'
        ]
        
        # Emotional variations
        self.emotional_variants = ['HAPPY', 'SERIOUS', 'EXCITED']
        
        # Contextual variations  
        self.contextual_variants = ['TEACH', 'QUESTION', 'EMPHASIS']
        
        # Output structure
        self.output_structure = {
            'basic': self.current_visemes,
            'enhanced': self.new_visemes,
            'emotional': [f"{v}_{e}" for v in ['A', 'E', 'I'] for e in self.emotional_variants],
            'contextual': [f"{v}_{c}" for v in ['A', 'E', 'I'] for c in self.contextual_variants]
        }
        
    def extract_training_frames(self, video_path: str, target_viseme: str) -> Optional[np.ndarray]:
        """
        Extract frames from training video that best represent a target viseme
        Uses audio analysis and visual detection to find optimal frames
        """
        try:
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                logger.error(f"Could not open video: {video_path}")
                return None
                
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            duration = total_frames / fps
            
            logger.info(f"Processing video: {video_path}")
            logger.info(f"Total frames: {total_frames}, FPS: {fps}, Duration: {duration:.2f}s")
            
            # Sample frames at strategic intervals
            sample_interval = max(1, total_frames // 50)  # Sample 50 frames
            frames = []
            
            for frame_idx in range(0, total_frames, sample_interval):
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                if ret:
                    frames.append((frame_idx, frame))
                    
            cap.release()
            
            if not frames:
                logger.warning(f"No frames extracted from {video_path}")
                return None
                
            # For now, return a middle frame as placeholder
            # In production, this would use AI to detect mouth shapes and select best matches
            middle_idx = len(frames) // 2
            selected_frame = frames[middle_idx][1]
            
            logger.info(f"Extracted {len(frames)} sample frames, selected frame {middle_idx}")
            return selected_frame
            
        except Exception as e:
            logger.error(f"Error extracting frames from {video_path}: {e}")
            return None
    
    def enhance_frame_quality(self, frame: np.ndarray, target_size: Tuple[int, int] = (512, 512)) -> np.ndarray:
        """
        Enhance frame quality using AI upscaling and standardization
        """
        try:
            # Resize to target size
            resized = cv2.resize(frame, target_size, interpolation=cv2.INTER_LANCZOS4)
            
            # Apply basic enhancement (in production, use AI upscaling)
            enhanced = cv2.detailEnhance(resized, sigma_s=10, sigma_r=0.15)
            
            # Normalize contrast
            lab = cv2.cvtColor(enhanced, cv2.COLOR_BGR2LAB)
            l, a, b = cv2.split(lab)
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
            l = clahe.apply(l)
            enhanced = cv2.merge([l, a, b])
            enhanced = cv2.cvtColor(enhanced, cv2.COLOR_LAB2BGR)
            
            return enhanced
            
        except Exception as e:
            logger.error(f"Error enhancing frame: {e}")
            return frame
    
    def generate_emotional_variants(self, base_frame: np.ndarray, emotion: str) -> np.ndarray:
        """
        Generate emotional variants of base viseme frames
        """
        try:
            # In production, use AI to generate emotional expressions
            # For now, apply basic visual effects
            if emotion == 'HAPPY':
                # Brighten and add warmth
                frame = cv2.convertScaleAbs(base_frame, alpha=1.1, beta=10)
            elif emotion == 'SERIOUS':
                # Darken slightly
                frame = cv2.convertScaleAbs(base_frame, alpha=0.9, beta=-5)
            elif emotion == 'EXCITED':
                # Increase saturation
                hsv = cv2.cvtColor(base_frame, cv2.COLOR_BGR2HSV)
                hsv[:, :, 1] = cv2.multiply(hsv[:, :, 1], 1.2)
                frame = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
            else:
                frame = base_frame.copy()
                
            return frame
            
        except Exception as e:
            logger.error(f"Error generating emotional variant {emotion}: {e}")
            return base_frame
    
    def generate_contextual_variants(self, base_frame: np.ndarray, context: str) -> np.ndarray:
        """
        Generate contextual variants for teaching scenarios
        """
        try:
            # In production, use AI to generate contextual mouth shapes
            # For now, apply subtle modifications
            if context == 'TEACH':
                # Slightly more pronounced mouth shape
                frame = cv2.convertScaleAbs(base_frame, alpha=1.05, beta=0)
            elif context == 'QUESTION':
                # Slightly raised eyebrows effect (simulated)
                frame = cv2.convertScaleAbs(base_frame, alpha=1.0, beta=5)
            elif context == 'EMPHASIS':
                # Enhanced contrast for emphasis
                frame = cv2.convertScaleAbs(base_frame, alpha=1.1, beta=0)
            else:
                frame = base_frame.copy()
                
            return frame
            
        except Exception as e:
            logger.error(f"Error generating contextual variant {context}: {e}")
            return base_frame
    
    def create_viseme_directory_structure(self, speaker: str):
        """Create the expanded directory structure for visemes"""
        base_dir = self.base_path / speaker / "2d"
        
        # Create main viseme directories
        for category in self.output_structure.keys():
            viseme_dir = base_dir / "visemes" / category
            viseme_dir.mkdir(parents=True, exist_ok=True)
            
        # Create metadata directory
        metadata_dir = base_dir / "metadata"
        metadata_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"Created directory structure for {speaker}")
    
    def generate_enhanced_visemes(self, speaker: str):
        """Generate all enhanced viseme frames for a speaker"""
        logger.info(f"Generating enhanced visemes for {speaker}")
        
        # Create directory structure
        self.create_viseme_directory_structure(speaker)
        
        # Process each video to extract base frames
        base_frames = {}
        for video_path in self.training_videos[speaker]:
            if os.path.exists(video_path):
                # Extract frames for new visemes
                for viseme in self.new_visemes:
                    if viseme not in base_frames:
                        frame = self.extract_training_frames(video_path, viseme)
                        if frame is not None:
                            base_frames[viseme] = frame
                            logger.info(f"Extracted base frame for {viseme} from {video_path}")
        
        # Generate enhanced visemes
        for viseme in self.new_visemes:
            if viseme in base_frames:
                # Enhance base frame
                enhanced = self.enhance_frame_quality(base_frames[viseme])
                
                # Save enhanced viseme
                output_path = self.base_path / speaker / "2d" / "visemes" / "enhanced" / f"mouth_{viseme}.png"
                cv2.imwrite(str(output_path), enhanced)
                logger.info(f"Generated enhanced viseme: {output_path}")
        
        # Generate emotional variants for key visemes
        key_visemes = ['A', 'E', 'I']
        for viseme in key_visemes:
            if viseme in base_frames:
                base_frame = base_frames.get(viseme, base_frames.get('A', list(base_frames.values())[0]))
                
                for emotion in self.emotional_variants:
                    emotional_frame = self.generate_emotional_variants(base_frame, emotion)
                    output_path = self.base_path / speaker / "2d" / "visemes" / "emotional" / f"mouth_{viseme}_{emotion}.png"
                    cv2.imwrite(str(output_path), emotional_frame)
                    logger.info(f"Generated emotional variant: {output_path}")
        
        # Generate contextual variants
        for viseme in key_visemes:
            if viseme in base_frames:
                base_frame = base_frames.get(viseme, base_frames.get('A', list(base_frames.values())[0]))
                
                for context in self.contextual_variants:
                    contextual_frame = self.generate_contextual_variants(base_frame, context)
                    output_path = self.base_path / speaker / "2d" / "visemes" / "contextual" / f"mouth_{viseme}_{context}.png"
                    cv2.imwrite(str(output_path), contextual_frame)
                    logger.info(f"Generated contextual variant: {output_path}")
    
    def update_rig_json(self, speaker: str):
        """Update rig.json to include all 48 visemes"""
        rig_path = self.base_path / speaker / "2d" / "rig.json"
        
        if not rig_path.exists():
            logger.error(f"Rig file not found: {rig_path}")
            return
        
        try:
            with open(rig_path, 'r') as f:
                rig_data = json.load(f)
            
            # Create comprehensive viseme list
            all_visemes = []
            for category, visemes in self.output_structure.items():
                all_visemes.extend(visemes)
            
            # Update rig.json
            rig_data['visemes'] = all_visemes
            rig_data['version'] = '2.0'
            rig_data['viseme_categories'] = self.output_structure
            
            # Save updated rig.json
            with open(rig_path, 'w') as f:
                json.dump(rig_data, f, indent=2)
            
            logger.info(f"Updated rig.json for {speaker} with {len(all_visemes)} visemes")
            
        except Exception as e:
            logger.error(f"Error updating rig.json for {speaker}: {e}")
    
    def create_viseme_mapping(self, speaker: str):
        """Create viseme mapping metadata"""
        mapping_data = {
            'speaker': speaker,
            'version': '2.0',
            'total_visemes': sum(len(visemes) for visemes in self.output_structure.values()),
            'categories': self.output_structure,
            'phoneme_mapping': {
                'REST': ['silence', 'pause'],
                'MBP': ['m', 'b', 'p'],
                'FV': ['f', 'v'],
                'TH': ['Œ∏', '√∞'],
                'DNTL': ['d', 'n', 't', 'l'],
                'KG': ['k', 'g', '≈ã'],
                'S': ['s', 'z'],
                'WQ': ['w', 'q'],
                'R': ['r', '…π'],
                'A': ['…ë', '√¶'],
                'E': ['…õ', 'e'],
                'I': ['i', '…™'],
                'OO': ['u', ' ä'],
                'UH': [' å', '…ô'],
                'AW': ['a ä'],
                'AY': ['a…™'],
                'OY': ['…î…™'],
                'NG': ['≈ã'],
                'CH': ['t É'],
                'SH': [' É'],
                'ZH': [' í'],
                'Y': ['j'],
                'L': ['l'],
                'H': ['h']
            }
        }
        
        output_path = self.base_path / speaker / "2d" / "metadata" / "viseme_mapping.json"
        with open(output_path, 'w') as f:
            json.dump(mapping_data, f, indent=2)
        
        logger.info(f"Created viseme mapping for {speaker}: {output_path}")
    
    def run_pipeline(self):
        """Run the complete viseme expansion pipeline"""
        logger.info("Starting Viseme Expansion Pipeline")
        
        for speaker in ['kelly', 'ken']:
            logger.info(f"\n{'='*50}")
            logger.info(f"Processing {speaker.upper()}")
            logger.info(f"{'='*50}")
            
            try:
                # Generate enhanced visemes
                self.generate_enhanced_visemes(speaker)
                
                # Update rig.json
                self.update_rig_json(speaker)
                
                # Create metadata
                self.create_viseme_mapping(speaker)
                
                logger.info(f"‚úÖ Completed processing for {speaker}")
                
            except Exception as e:
                logger.error(f"‚ùå Error processing {speaker}: {e}")
        
        logger.info("\nüéâ Viseme Expansion Pipeline Complete!")
        self.generate_summary_report()
    
    def generate_summary_report(self):
        """Generate a summary report of the expansion"""
        report = {
            'timestamp': str(np.datetime64('now')),
            'pipeline_version': '2.0',
            'expansion_results': {}
        }
        
        for speaker in ['kelly', 'ken']:
            speaker_report = {
                'original_visemes': len(self.current_visemes),
                'new_visemes': len(self.new_visemes),
                'emotional_variants': len(self.output_structure['emotional']),
                'contextual_variants': len(self.output_structure['contextual']),
                'total_visemes': sum(len(visemes) for visemes in self.output_structure.values()),
                'output_directories': [
                    str(self.base_path / speaker / "2d" / "visemes" / category)
                    for category in self.output_structure.keys()
                ]
            }
            report['expansion_results'][speaker] = speaker_report
        
        # Save report
        report_path = self.base_path / "viseme_expansion_report.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        logger.info(f"üìä Summary report saved to: {report_path}")
        
        # Print summary
        print("\n" + "="*60)
        print("üé≠ VISEME EXPANSION SUMMARY")
        print("="*60)
        for speaker, results in report['expansion_results'].items():
            print(f"\n{speaker.upper()}:")
            print(f"  Original: {results['original_visemes']} visemes")
            print(f"  Enhanced: {results['new_visemes']} visemes") 
            print(f"  Emotional: {results['emotional_variants']} variants")
            print(f"  Contextual: {results['contextual_variants']} variants")
            print(f"  TOTAL: {results['total_visemes']} visemes")
        print("\n" + "="*60)

def main():
    parser = argparse.ArgumentParser(description='Viseme Expansion Pipeline for Kelly & Ken')
    parser.add_argument('--base-path', default='production-deploy/assets/avatars',
                       help='Base path for avatar assets')
    parser.add_argument('--speaker', choices=['kelly', 'ken', 'both'], default='both',
                       help='Which speaker to process')
    
    args = parser.parse_args()
    
    pipeline = VisemeExpansionPipeline(args.base_path)
    
    if args.speaker == 'both':
        pipeline.run_pipeline()
    else:
        # Process single speaker
        logger.info(f"Processing {args.speaker} only")
        pipeline.generate_enhanced_visemes(args.speaker)
        pipeline.update_rig_json(args.speaker)
        pipeline.create_viseme_mapping(args.speaker)
        pipeline.generate_summary_report()

if __name__ == "__main__":
    main()
